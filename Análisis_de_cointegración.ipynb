{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iLBzf6FqwE6f",
        "outputId": "36fc9bd9-46ce-4bbc-f877-a38d371a90ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Instalando librerías necesarias...\n",
            "Librerías cargadas\n",
            "\n",
            "2) Por favor SUBE tu archivo 'Datos.xlsx' (columnas: Fecha, IPSA, COLCAP).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-176b64a6-f66e-4570-916f-e467c457fa25\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-176b64a6-f66e-4570-916f-e467c457fa25\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Datos.xlsx to Datos (1).xlsx\n",
            "Archivo subido: Datos (1).xlsx\n",
            "Datos históricos: 4162 filas. Desde 2008-05-23 hasta 2025-12-01\n",
            "\n",
            "3) Iniciando SparkSession...\n",
            "Spark version: 3.5.1\n",
            "\n",
            "4) Análisis batch (ADF en niveles y retornos, Engle-Granger)\n",
            " ADF IPSA (nivel): stat=-2.0961, p=0.246095, crit={'1%': np.float64(-3.4319282328371266), '5%': np.float64(-2.8622373766412523), '10%': np.float64(-2.567141219855403)}\n",
            " ADF COLCAP (nivel): stat=-1.5563, p=0.505484, crit={'1%': np.float64(-3.431923675236044), '5%': np.float64(-2.8622353632982076), '10%': np.float64(-2.5671401480435727)}\n",
            " ADF IPSA (retornos): stat=-17.0172, p=0.000000, crit={'1%': np.float64(-3.431927852028984), '5%': np.float64(-2.862237208417471), '10%': np.float64(-2.5671411303007297)}\n",
            " ADF COLCAP (retornos): stat=-40.1289, p=0.000000, crit={'1%': np.float64(-3.4319232966238484), '5%': np.float64(-2.8622351960442405), '10%': np.float64(-2.5671400590052276)}\n",
            "\n",
            "Engle-Granger: stat=-1.9503, p-value=0.554288, crit=[-3.89907461 -3.33759917 -3.04546968]\n",
            "¿Cointegración histórica?  False\n",
            "\n",
            "5) Preparando carpeta para Spark Structured Streaming (simulación API)...\n",
            "Carpeta creada: /content/stream_api/\n",
            "Buffer inicial: 4161 observaciones\n",
            "\n",
            "8) Configurando Spark Structured Streaming (leyendo JSON desde carpeta)...\n",
            "Streaming iniciado. Esperando archivos en: /content/stream_api/\n",
            "\n",
            "9) Iniciando simulación de API que escribe nuevos ticks cada 5s (10 iteraciones).\n",
            "Sim API -> creado: /content/stream_api/tick_0_c8a4220f14834223a7d86b310fe800bd.json  [2025-12-02] (IPSA=10.9094, COLCAP=0.5455)\n",
            "\n",
            "[batch 0] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018782)\n",
            " Cointegration last 250: stat=-2.5806805530365913, p=0.24469026068325628\n",
            "Sim API -> creado: /content/stream_api/tick_1_d80168c83c6048449c6cf67ac6898cd3.json  [2025-12-03] (IPSA=10.9487, COLCAP=0.5467)\n",
            "\n",
            "[batch 1] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018769)\n",
            " Cointegration last 250: stat=-2.570120004558864, p=0.24904919128491504\n",
            "Sim API -> creado: /content/stream_api/tick_2_389519c0a1424bd6a25cad05e9ab87b8.json  [2025-12-04] (IPSA=10.9381, COLCAP=0.5460)\n",
            "\n",
            "[batch 2] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018774)\n",
            " Cointegration last 250: stat=-2.5812463613325676, p=0.24445795542235021\n",
            "Sim API -> creado: /content/stream_api/tick_3_6aed5e35fd754a76bf37e53af83e42a9.json  [2025-12-05] (IPSA=10.9658, COLCAP=0.5473)\n",
            "\n",
            "[batch 3] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018760)\n",
            " Cointegration last 250: stat=-2.574708829783759, p=0.2471497592533607\n",
            "Sim API -> creado: /content/stream_api/tick_4_8bafe667fab24794a9de716aea5615dd.json  [2025-12-06] (IPSA=10.9329, COLCAP=0.5443)\n",
            "\n",
            "[batch 4] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018766)\n",
            " Cointegration last 250: stat=-2.613411852231474, p=0.2314611594695986\n",
            "Sim API -> creado: /content/stream_api/tick_5_9a22db24970c4a08bca6ee2fa22b31df.json  [2025-12-07] (IPSA=11.0061, COLCAP=0.5464)\n",
            "\n",
            "[batch 5] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018763)\n",
            " Cointegration last 250: stat=-2.5947857850245124, p=0.2389368242089095\n",
            "Sim API -> creado: /content/stream_api/tick_6_d93f51959e2141b485b696b105ddc162.json  [2025-12-08] (IPSA=10.9349, COLCAP=0.5460)\n",
            "\n",
            "[batch 6] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018764)\n",
            " Cointegration last 250: stat=-2.5936175567934443, p=0.23941033570119508\n",
            "Sim API -> creado: /content/stream_api/tick_7_55160f58d03e4784a99d64bb5c2de5b1.json  [2025-12-09] (IPSA=10.9077, COLCAP=0.5465)\n",
            "\n",
            "[batch 7] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018760)\n",
            " Cointegration last 250: stat=-2.629298903254106, p=0.22602994947839938\n",
            "Sim API -> creado: /content/stream_api/tick_8_75924856875a46528348f93fd7c89628.json  [2025-12-10] (IPSA=10.8979, COLCAP=0.5477)\n",
            "\n",
            "[batch 8] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018765)\n",
            " Cointegration last 250: stat=-2.618473255284382, p=0.22945394979232542\n",
            "Sim API -> creado: /content/stream_api/tick_9_889c3ea43859406f92d74a5faf77c82d.json  [2025-12-11] (IPSA=10.9136, COLCAP=0.5485)\n",
            "\n",
            "[batch 9] Procesadas 1 observaciones. Buffer total: 1500\n",
            " RollingCorr(60) = 0.4370\n",
            " Volatility IPA recent/std = 0.012273 (hist=0.018771)\n",
            " Cointegration last 250: stat=-2.582428297420217, p=0.2439730921791997\n",
            "\n",
            "Simulación completada. Esperando procesamiento final (15s)...\n",
            "\n",
            "Deteniendo streaming query...\n",
            "Streaming detenido.\n",
            "\n",
            "Últimas 8 observaciones del buffer (incluye histórico + stream):\n",
            "          Fecha       IPSA    COLCAP\n",
            "1492 2025-12-04  10.938102  0.545952\n",
            "1493 2025-12-05  10.965816  0.547275\n",
            "1494 2025-12-06  10.932910  0.544344\n",
            "1495 2025-12-07  11.006101  0.546391\n",
            "1496 2025-12-08  10.934887  0.546026\n",
            "1497 2025-12-09  10.907666  0.546477\n",
            "1498 2025-12-10  10.897886  0.547683\n",
            "1499 2025-12-11  10.913649  0.548539\n",
            "\n",
            "Test final Engle-Granger sobre ventana usada: stat=-2.5824, p-value=0.243973\n",
            "Estado cointegración final: False\n",
            "\n",
            "Spark detenido. ANÁLISIS COMPLETO FINALIZADO.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Notebook: Batch + Spark Structured Streaming\n",
        "# ============================================================\n",
        "\n",
        "# ---------------------------\n",
        "# 0) Instalar dependencias\n",
        "# ---------------------------\n",
        "print(\"1) Instalando librerías necesarias...\")\n",
        "!pip install pyspark==3.5.1 statsmodels openpyxl pandas numpy matplotlib --quiet\n",
        "\n",
        "# ---------------------------\n",
        "# 1) Librerías\n",
        "# ---------------------------\n",
        "import os, time, uuid, threading\n",
        "from datetime import timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, TimestampType, DoubleType\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller, coint\n",
        "import statsmodels.api as sm\n",
        "\n",
        "print(\"Librerías cargadas\")\n",
        "\n",
        "# ---------------------------\n",
        "# 2) Pedir archivo y leer Excel\n",
        "# ---------------------------\n",
        "from google.colab import files\n",
        "print(\"\\n2) Por favor SUBE tu archivo 'Datos.xlsx' (columnas: Fecha, IPSA, COLCAP).\")\n",
        "uploaded = files.upload()  # sube el archivo cuando aparezca el diálogo\n",
        "file_name = list(uploaded.keys())[0]\n",
        "print(\"Archivo subido:\", file_name)\n",
        "\n",
        "# leer con pandas\n",
        "df = pd.read_excel(file_name)\n",
        "# normalizar nombres\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "expected = [\"Fecha\", \"IPSA\", \"COLCAP\"]\n",
        "if not all([name in df.columns for name in expected]):\n",
        "    raise ValueError(f\"El archivo debe contener las columnas: {expected}. Columnas encontradas: {list(df.columns)}\")\n",
        "\n",
        "df['Fecha'] = pd.to_datetime(df['Fecha'])\n",
        "df = df.sort_values('Fecha').reset_index(drop=True)\n",
        "print(f\"Datos históricos: {len(df)} filas. Desde {df['Fecha'].min().date()} hasta {df['Fecha'].max().date()}\")\n",
        "\n",
        "# ---------------------------\n",
        "# 3) Iniciar SparkSession\n",
        "# ---------------------------\n",
        "print(\"\\n3) Iniciando SparkSession...\")\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IPSA_COLCAP_Batch_Streaming\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "# ---------------------------\n",
        "# 4) Análisis BATCH inicial\n",
        "# ---------------------------\n",
        "print(\"\\n4) Análisis batch (ADF en niveles y retornos, Engle-Granger)\")\n",
        "\n",
        "# log-prices and returns\n",
        "df['Log_IPSA'] = np.log(df['IPSA'])\n",
        "df['Log_COLCAP'] = np.log(df['COLCAP'])\n",
        "df['rIPSA'] = df['Log_IPSA'].diff()\n",
        "df['rCOLCAP'] = df['Log_COLCAP'].diff()\n",
        "df_clean = df.dropna().copy()\n",
        "\n",
        "def run_adf(series, name):\n",
        "    stat, pval, _, _, crit, _ = adfuller(series.dropna())\n",
        "    print(f\" ADF {name}: stat={stat:.4f}, p={pval:.6f}, crit={crit}\")\n",
        "    return stat, pval\n",
        "\n",
        "# ADF niveles\n",
        "run_adf(df_clean['Log_IPSA'], \"IPSA (nivel)\")\n",
        "run_adf(df_clean['Log_COLCAP'], \"COLCAP (nivel)\")\n",
        "# ADF retornos\n",
        "run_adf(df_clean['rIPSA'], \"IPSA (retornos)\")\n",
        "run_adf(df_clean['rCOLCAP'], \"COLCAP (retornos)\")\n",
        "\n",
        "# Engle-Granger cointegration on log-prices\n",
        "y = df_clean['Log_IPSA'].values\n",
        "x = df_clean['Log_COLCAP'].values\n",
        "eg_stat, eg_p, eg_crit = coint(y, x)\n",
        "print(f\"\\nEngle-Granger: stat={eg_stat:.4f}, p-value={eg_p:.6f}, crit={eg_crit}\")\n",
        "initial_cointegrated = (eg_p < 0.05)\n",
        "print(\"¿Cointegración histórica? \", initial_cointegrated)\n",
        "\n",
        "# ---------------------------\n",
        "# 5) Preparar carpeta de streaming (API simulada escribirá JSON)\n",
        "# ---------------------------\n",
        "print(\"\\n5) Preparando carpeta para Spark Structured Streaming (simulación API)...\")\n",
        "stream_dir = \"/content/stream_api/\"\n",
        "if os.path.exists(stream_dir):\n",
        "    # limpiar para evitar archivos viejos\n",
        "    import shutil\n",
        "    shutil.rmtree(stream_dir)\n",
        "os.makedirs(stream_dir, exist_ok=True)\n",
        "print(\"Carpeta creada:\", stream_dir)\n",
        "\n",
        "# Definir schema para lectura de JSON\n",
        "schema = StructType([\n",
        "    StructField(\"Fecha\", TimestampType(), True),\n",
        "    StructField(\"IPSA\", DoubleType(), True),\n",
        "    StructField(\"COLCAP\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# 6) Buffer en driver para almacenar datos \"reales + stream\"\n",
        "# ---------------------------\n",
        "# empezamos con los datos históricos ya en df_buffer\n",
        "df_buffer = df_clean[['Fecha','IPSA','COLCAP','Log_IPSA','Log_COLCAP','rIPSA','rCOLCAP']].copy()\n",
        "df_buffer = df_buffer.reset_index(drop=True)\n",
        "\n",
        "# parámetros de vigilancia\n",
        "ROLLING_WINDOW = 60   # nº de observaciones para rolling correlation (en días)\n",
        "COINT_WINDOW = 250    # ventana para re-ejecutar test de cointegración (últimos N obs)\n",
        "CORR_ALERT_LOW = 0.3  # umbral baja correlación\n",
        "VOL_ALERT_MULT = 1.5  # si vol actual > 1.5 * vol historial -> alerta\n",
        "\n",
        "print(f\"Buffer inicial: {len(df_buffer)} observaciones\")\n",
        "\n",
        "# ---------------------------\n",
        "# 7) Función que procesa cada micro-batch\n",
        "# ---------------------------\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import lag, log as spark_log\n",
        "\n",
        "# global variables referenced inside foreachBatch\n",
        "global_state = {\n",
        "    \"df_buffer\": df_buffer,\n",
        "    \"initial_cointegrated\": initial_cointegrated\n",
        "}\n",
        "\n",
        "def process_microbatch(microbatch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Esta función se ejecuta en el driver para cada micro-batch.\n",
        "    microbatch_df: Spark DataFrame (micro-batch)\n",
        "    batch_id: id del batch\n",
        "    \"\"\"\n",
        "    if microbatch_df.rdd.isEmpty():\n",
        "        print(f\"[batch {batch_id}] microbatch vacío\")\n",
        "        return\n",
        "\n",
        "    # convertir a pandas\n",
        "    pdf = microbatch_df.toPandas()\n",
        "    pdf = pdf.sort_values(\"Fecha\").reset_index(drop=True)\n",
        "\n",
        "    # calcular log y retornos en el microbatch\n",
        "    pdf['Log_IPSA'] = np.log(pdf['IPSA'])\n",
        "    pdf['Log_COLCAP'] = np.log(pdf['COLCAP'])\n",
        "    pdf['rIPSA'] = pdf['Log_IPSA'].diff()\n",
        "    pdf['rCOLCAP'] = pdf['Log_COLCAP'].diff()\n",
        "\n",
        "    # append to global buffer\n",
        "    buf = global_state['df_buffer']\n",
        "    buf = pd.concat([buf, pdf[['Fecha','IPSA','COLCAP','Log_IPSA','Log_COLCAP','rIPSA','rCOLCAP']]], ignore_index=True)\n",
        "    buf = buf.drop_duplicates(subset=['Fecha']).sort_values('Fecha').reset_index(drop=True)\n",
        "\n",
        "    # keep a reasonable maximum length to avoid memoria infinita (por ejemplo 5 años * ~252 = 1260)\n",
        "    maxlen = max(COINT_WINDOW*2, 1500)\n",
        "    if len(buf) > maxlen:\n",
        "        buf = buf.iloc[-maxlen:].reset_index(drop=True)\n",
        "\n",
        "    # rolling correlation (last ROLLING_WINDOW)\n",
        "    if len(buf) >= ROLLING_WINDOW:\n",
        "        recent = buf[['rIPSA','rCOLCAP']].dropna().iloc[-ROLLING_WINDOW:]\n",
        "        rolling_corr = recent['rIPSA'].corr(recent['rCOLCAP'])\n",
        "    else:\n",
        "        rolling_corr = np.nan\n",
        "\n",
        "    # volatility comparison: recent vs historical (std of returns)\n",
        "    hist_vol = buf['rIPSA'].std()  # global vol (IPSA)\n",
        "    recent_vol = buf['rIPSA'].dropna().iloc[-ROLLING_WINDOW:].std() if len(buf.dropna())>=ROLLING_WINDOW else np.nan\n",
        "\n",
        "    # cointegration on last COINT_WINDOW observations (log prices)\n",
        "    has_cointegr = False\n",
        "    if len(buf) >= COINT_WINDOW:\n",
        "        Yc = buf['Log_IPSA'].values[-COINT_WINDOW:]\n",
        "        Xc = buf['Log_COLCAP'].values[-COINT_WINDOW:]\n",
        "        try:\n",
        "            statc, pc, _ = coint(Yc, Xc)\n",
        "            has_cointegr = (pc < 0.05)\n",
        "        except Exception as e:\n",
        "            print(\"Error cointegration test:\", e)\n",
        "            statc, pc = np.nan, np.nan\n",
        "    else:\n",
        "        statc, pc = np.nan, np.nan\n",
        "\n",
        "    # print summary and possible alerts\n",
        "    print(f\"\\n[batch {batch_id}] Procesadas {len(pdf)} observaciones. Buffer total: {len(buf)}\")\n",
        "    print(f\" RollingCorr({ROLLING_WINDOW}) = {rolling_corr:.4f}\")\n",
        "    if not np.isnan(recent_vol) and not np.isnan(hist_vol):\n",
        "        print(f\" Volatility IPA recent/std = {recent_vol:.6f} (hist={hist_vol:.6f})\")\n",
        "    print(f\" Cointegration last {COINT_WINDOW}: stat={statc}, p={pc}\")\n",
        "\n",
        "    # alertas\n",
        "    if not np.isnan(rolling_corr) and rolling_corr < CORR_ALERT_LOW:\n",
        "        print(\"!!! ALERTA: Correlación rolling baja <\", CORR_ALERT_LOW)\n",
        "\n",
        "    if not np.isnan(recent_vol) and not np.isnan(hist_vol) and recent_vol > VOL_ALERT_MULT * hist_vol:\n",
        "        print(\"!!! ALERTA: Volatilidad reciente > {VOL_ALERT_MULT} * volatilidad histórica\")\n",
        "\n",
        "    # detect change in cointegration state vs initial\n",
        "    prev_state = global_state.get('initial_cointegrated', False)\n",
        "    if not np.isnan(pc):\n",
        "        if (pc < 0.05) and (not prev_state):\n",
        "            print(\"+++ ALERTA: Cointegración APARECIÓ en la ventana reciente\")\n",
        "            global_state['initial_cointegrated'] = True\n",
        "        elif (pc >= 0.05) and prev_state:\n",
        "            print(\"--- ALERTA: Cointegración DESAPARECIÓ en la ventana reciente\")\n",
        "            global_state['initial_cointegrated'] = False\n",
        "\n",
        "    # update buffer\n",
        "    global_state['df_buffer'] = buf\n",
        "\n",
        "# ---------------------------\n",
        "# 8) Crear streaming DataFrame y arrancar query con foreachBatch\n",
        "# ---------------------------\n",
        "print(\"\\n8) Configurando Spark Structured Streaming (leyendo JSON desde carpeta)...\")\n",
        "\n",
        "streaming_df = spark.readStream.schema(schema).option(\"maxFilesPerTrigger\", 1).json(stream_dir)\n",
        "\n",
        "# start streaming with foreachBatch\n",
        "query = streaming_df.writeStream.foreachBatch(process_microbatch).option(\"checkpointLocation\",\"/content/checkpoint_ipsc_colcap\").trigger(processingTime='5 seconds').start()\n",
        "print(\"Streaming iniciado. Esperando archivos en:\", stream_dir)\n",
        "\n",
        "# ---------------------------\n",
        "# 9) API simulada: escribimos N archivos JSON a la carpeta stream_dir\n",
        "# ---------------------------\n",
        "print(\"\\n9) Iniciando simulación de API que escribe nuevos ticks cada 5s (10 iteraciones).\")\n",
        "# We'll simulate 10 new \"days\" arriving one every 5 seconds\n",
        "N_SIM = 10\n",
        "interval_secs = 5\n",
        "\n",
        "# start date = last date in buffer\n",
        "last_date = global_state['df_buffer']['Fecha'].iloc[-1]\n",
        "\n",
        "# use last prices as seed\n",
        "last_ipsa = global_state['df_buffer']['IPSA'].iloc[-1]\n",
        "last_col = global_state['df_buffer']['COLCAP'].iloc[-1]\n",
        "\n",
        "for i in range(N_SIM):\n",
        "    new_date = last_date + timedelta(days=1 + i)\n",
        "    # simulate small random walk return\n",
        "    new_ipsa = float(last_ipsa * (1 + np.random.normal(0, 0.002)))\n",
        "    new_col = float(last_col * (1 + np.random.normal(0, 0.002)))\n",
        "    rec = pd.DataFrame([{\"Fecha\": new_date, \"IPSA\": new_ipsa, \"COLCAP\": new_col}])\n",
        "    # write json file to stream_dir; Spark will pick it up\n",
        "    file_path = os.path.join(stream_dir, f\"tick_{i}_{uuid.uuid4().hex}.json\")\n",
        "    rec.to_json(file_path, orient='records', date_format='iso')\n",
        "    print(f\"Sim API -> creado: {file_path}  [{new_date.date()}] (IPSA={new_ipsa:.4f}, COLCAP={new_col:.4f})\")\n",
        "    time.sleep(interval_secs)\n",
        "\n",
        "# Allow some time for final micro-batches to be processed\n",
        "print(\"\\nSimulación completada. Esperando procesamiento final (15s)...\")\n",
        "time.sleep(15)\n",
        "\n",
        "# stop query\n",
        "print(\"\\nDeteniendo streaming query...\")\n",
        "query.stop()\n",
        "print(\"Streaming detenido.\")\n",
        "\n",
        "# ---------------------------\n",
        "# 10) Resultado final: mostrar últimas filas del buffer y re-ejecutar test de cointegración final\n",
        "# ---------------------------\n",
        "final_buf = global_state['df_buffer'].copy()\n",
        "print(\"\\nÚltimas 8 observaciones del buffer (incluye histórico + stream):\")\n",
        "print(final_buf[['Fecha','IPSA','COLCAP']].tail(8))\n",
        "\n",
        "if len(final_buf) >= COINT_WINDOW:\n",
        "    Yf = final_buf['Log_IPSA'].values[-COINT_WINDOW:]\n",
        "    Xf = final_buf['Log_COLCAP'].values[-COINT_WINDOW:]\n",
        "else:\n",
        "    Yf = final_buf['Log_IPSA'].values\n",
        "    Xf = final_buf['Log_COLCAP'].values\n",
        "\n",
        "statf, pf, _ = coint(Yf, Xf)\n",
        "print(f\"\\nTest final Engle-Granger sobre ventana usada: stat={statf:.4f}, p-value={pf:.6f}\")\n",
        "print(\"Estado cointegración final:\", (pf < 0.05))\n",
        "\n",
        "# ---------------------------\n",
        "# 11) Cerrar Spark\n",
        "# ---------------------------\n",
        "spark.stop()\n",
        "print(\"\\nSpark detenido. ANÁLISIS COMPLETO FINALIZADO.\")\n"
      ]
    }
  ]
}